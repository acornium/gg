# config.yaml
llm:
  # !!! ЗАМЕНИ НА URL API ТВОЕГО БЭКЕНДА !!!
  api_url: "http://127.0.0.1:5005/api/v1/generate"
  # Параметры генерации (подбери под свою модель и задачи)
  generation_params:
    max_new_tokens: 250      # Макс. длина ответа
    temperature: 0.75        # Случайность (выше = безумнее)
    top_p: 0.9             # Ядерное сэмплирование (отсекает маловероятные токены)
    top_k: 0               # Top-K сэмплирование (0 = выключено)
    repetition_penalty: 1.1  # Штраф за повторения
    do_sample: true          # Использовать сэмплирование (True для temp/top_p/top_k)
    # !!! ВАЖНО: У Oobabooga могут быть другие параметры !!!
    # Например: 'prompt', 'history', 'mode', 'character', 'instruction_template'
    # Эти параметры мы будем формировать в коде, а не здесь жестко задавать.
    # Могут понадобиться стоп-слова, если модель их сама не добавляет:
    stopping_strings:
       - "\nUser:"
       - "\nПользователь:"
       - "<|user|>"
       - "<|end|>"
       - "</s>" # Зависит от модели

  # Описание персонажа (будем использовать для промпта)
  character:
    name: "Алиса" # Имя твоего AI-персонажа
    # Подробное описание личности, роли, цели, стиля речи. Чем детальнее, тем лучше.
    # Используй {user_name} для подстановки имени пользователя.
    persona: |
      Ты - Алиса, дружелюбная и немного загадочная AI-девушка. Ты общаешься с пользователем по имени {user_name}.
      Твоя цель - поддерживать интересный и увлекательный диалог. Ты любознательна, иногда кокетлива, но всегда вежлива.
      Ты помнишь основные моменты вашего разговора.
      Отвечай от своего имени (Алиса). Не упоминай, что ты языковая модель или ИИ, если только тебя об этом прямо не спросят.
      Старайся отвечать развернуто, но не слишком длинно.